{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "executions = range(1, 6) # amount of executions per raha configuration\n",
    "# labeling_budget = [0.10, 0.25, 0.5, 0.75, 1, 2, 3, 5, 10, 15, 20]\n",
    "n_cols = 1343 # amount of columns in the data lake\n",
    "# labeling_budget_cells = [round(n_cols*x) for x in labeling_budget]\n",
    "labeling_budget_cells = [158, 336, 672, 1008] # amount of labeling budgets Raha was run with (this row or the row above)\n",
    "sandbox_path = \"./raha/raha/datasets/DGov-141\" # path towards the data lake (this dataset is not in this repository, but in another)\n",
    "res_path = \"./results/raha-dgov-141/exp_raha-non-enough-labels-dgov-141\" # path towards the experiment folder, where the results are stored\n",
    "df_path = \"./raha/benchmark-results/DGov-141/raha-dgov-141-rt.csv\" # path towards a csv file that contain benchmark results\n",
    "tp_fn = 1082998 # amount of true positives and false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = {\"labeling_budget\": [], \"precision\": [], \"recall\": [], \"f_score\": []}\n",
    "for l in labeling_budget_cells:\n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f_score = 0\n",
    "    for exec in executions:\n",
    "        tp = 0\n",
    "        tp_fp = 0\n",
    "        results_path_raha = os.path.join(res_path, \"results_\" + str(l) + \"_\"  + str(exec))\n",
    "        for file in os.listdir(results_path_raha):\n",
    "            if file.endswith(\".json\"):\n",
    "                with open(os.path.join(results_path_raha, file)) as f:\n",
    "                    json_content = json.load(f)\n",
    "                    tp += json_content['tp']\n",
    "                    tp_fp += json_content['ed_tpfp']\n",
    "        precision = tp / tp_fp if tp_fp > 0 else 0\n",
    "        recall = tp / tp_fn if tp_fn > 0 else 0\n",
    "        f_score = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f_score += f_score\n",
    "    avg_precision = total_precision / len(executions)\n",
    "    avg_recall = total_recall / len(executions)\n",
    "    avg_f_score = total_f_score / len(executions)\n",
    "    results[\"labeling_budget\"].append(l)\n",
    "    results[\"precision\"].append(avg_precision)\n",
    "    results[\"recall\"].append(avg_recall)\n",
    "    results[\"f_score\"].append(avg_f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(df_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Benchmarks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
